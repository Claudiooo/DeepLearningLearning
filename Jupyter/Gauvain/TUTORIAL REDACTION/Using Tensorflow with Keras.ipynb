{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  A little introduction to Keras, using TensorFlow\n",
    "    \n",
    " Keras is a simple and high-level definition interface \n",
    " that make the use of TensorFlow more easier<br><br>\n",
    "\n",
    "<font color=red size=2 ><B><i>!! This tutorial assumes that you have configured Keras\n",
    " to use the TensorFlow backend</font><B></i></br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    \n",
    "## Part One : Calling Keras layers on TensorFlow tensors\n",
    "\n",
    " __We will start with a simple example of MNIST digits classification.<br>\n",
    " We will build a TensorFlow digits classifier using a stack of Keras Dense layers__\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to create a Tensorflow session and registering it with Keras.<br>\n",
    "* Keras will use this session to initialize all internal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now building our classifier exactly like we would do with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder that will contain our input digits, as flat vectors\n",
    "img = tf.placeholder(tf.float32, shape=(None, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Keras layers can be called on TensorFlow tensors :<br><br>\n",
    " *Using Keras layers will speed up the model definition process*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# Fully-connected layer with 128 unis\n",
    "# and ReLU activation (Rectified Linear Unit)\n",
    "x = Dense(128, activation='relu')(img)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "\n",
    "# Output layer with 10 units and a softmax activation\n",
    "preds = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the placeholder for the labels and the loss function to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "from keras.objectives import categorical_crossentropy\n",
    "loss = tf.reduce_mean(categorical_crossentropy(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training part, with a TensorFlow optimizer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "#Initialization of all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "#Run training loop\n",
    "with sess.as_default():\n",
    "    for i in range(100):\n",
    "        batch = mnist_data.train.next_batch(50)\n",
    "        train_step.run(feed_dict={img: batch[0], labels: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import categorical_accuracy as accuracy\n",
    "acc_value = accuracy(labels, preds)\n",
    "with sess.as_default():\n",
    "    print (acc_value.eval(feed_dict={img: mnist_data.test.images,\n",
    "                                    labels: mnist_data.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used Keras as a syntaxical shortcut to generate an operator that just map some input tensors to some output tensors.\n",
    "The optimization is done via a TensorFlow native optimizer rather than a Keras one and no Keras model have been used.<br><br>\n",
    "\n",
    "Concerning the optimizers performances, there is little speed differences between Keras and TensorFlow use. Keras seems to be faster than TensorFlow nativ optimizer on most cases, but the differences is only from 5 to 10%.\n",
    "So for most of the projects, it doesn't really matter which one you choose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some different behaviors during training and testing phases\n",
    "\n",
    "Some Keras layers (like Dropout or BatchNormalization) have different behaviors at training time and testing time.<br>\n",
    "It's possible to know if a layer uses the \"leaarning phase\" (train/test) by printin the layer.uses_learning_phase variable. A boolean that is set to True if the layer has a different behavior on training mode and testin mode, and False otherwise.<br><br>\n",
    "\n",
    "If the model use such layers, you need to specify the value of the learning phase as part of feed_dict. That way the model knows wether to apply the layer.<br><br>\n",
    "\n",
    "The Keras learnin phase is accessible via the Keras backend. <br>\n",
    "Like this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"keras_learning_phase:0\", dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "print (K.learning_phase())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the use of learning phase, write the value 1 for training mode or 0 for testing mode, to feed_dict :<br>\n",
    "*(here we have added \"session=sess\" because run() need a default session to work)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train mode\n",
    "train_step.run(feed_dict={img: batch[0], labels: batch[1],\n",
    "                          K.learning_phase(): 1}, session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code to add the Dropout layer to our previous MNIST example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "\n",
    "img = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "x = Dense(128, activation='relu')(img)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(10, activation='softmax')(x)\n",
    "\n",
    "loss = tf.reduce_mean(categorical_crossentropy(labels, preds))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init_op)\n",
    "with sess.as_default():\n",
    "    for i in range(100):\n",
    "        batch = mnist_data.train.next_batch(50)\n",
    "        train_step.run(feed_dict={img: batch[0],\n",
    "                                  labels: batch[1],\n",
    "                                  K.learning_phase(): 1})\n",
    "\n",
    "acc_value = accuracy(labels, preds)\n",
    "with sess.as_default():\n",
    "    print (acc_value.eval(feed_dict={img: mnist_data.test.images,\n",
    "                                    labels: mnist_data.test.labels,\n",
    "                                    K.learning_phase(): 0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! *Nexts codes snipets are here to explain the usage. But if you execute them you need to include theses :*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility with name scopes and device scopes\n",
    "<br>\n",
    "Keras layers and models are fully compatible with TensorFlow name scopes. For instance, consider the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "with tf.name_scope('block1'):\n",
    "    y = LSTM(32, name='mylstm')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of our new LSTM layer will now be named block1/mylstm_W_i, block1/mylstm_U_i, and so on...\n",
    "\n",
    "Similarly, devices scopes would work like this :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you use the gpu version of Tensorflow use this '/gpu:0' instead of this\n",
    "with tf.device('/cpu:0'):\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "    y = LSTM(32)(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility with graph scopes\n",
    "\n",
    "Any Keras layer or model that you define inside a TensorFlow graph scope will be created as part of the specified graph, with all he's variables an operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "\n",
    "my_graph = tf.Graph()\n",
    "with my_graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "    # LSTM operators and variables are part of the Graph\n",
    "    y = LSTM(32)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility with variables scopes\n",
    "\n",
    "Variables sharing should be done via calling multiple instance of the same Keras layer, because the TenorFlow variable scope will have NO effect on a Keras model or layer.<br>\n",
    "More informations on wheight sharing with Keras could be found on the \"Wheight Sharing\" section of the functional API guide.<br><br>\n",
    "\n",
    "Here is an exemple of weight sharing by reusing the same layer instance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Keras layer\n",
    "lstm = LSTM(32)\n",
    "\n",
    "# Instantiate 2 TensorFlow placeholders\n",
    "x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "\n",
    "# Encoding the 2 tensors with the same LSTM weights\n",
    "x_encoded = lstm(x)\n",
    "y_encoded = lstm(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colleting trainable weights and state updates\n",
    "\n",
    "When using keras as a simplified interface to tensorflow, some Keras layers (stateful RNNs and BatchNormalization layers) have internal updates that need to be run as part of each training step. There are stored as a list of tensor tuples, layer.updates. We should generate assign operators for those, to be run at each training step.<br>Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Instanciate a layer\n",
    "layer = BatchNormalization()\n",
    "\n",
    "# Call a layer instance\n",
    "blah = layer(x)\n",
    "\n",
    "# Generate assign operators\n",
    "update_ops = []\n",
    "for assign_op in layer.updates:\n",
    "    update_ops.append(assign_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you are using a Keras Model instead of a layer, model.updates will behaves in the same way, and collects the updates of all the underlaying layers in the model.<br><br>\n",
    "In addition, if you want to explicitly collect a layer's trainable weights, you can do so using \"layer.trainable_weights\" (or \"model.trainable_weights\"), a list of TensorFlow Variable instances :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_7/kernel:0' shape=(64, 32) dtype=float32_ref>, <tf.Variable 'dense_7/bias:0' shape=(32,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# Instanciate a layer\n",
    "layer = Dense(32)\n",
    "\n",
    "# Call a layer instance\n",
    "blah = layer(x)\n",
    "\n",
    "# List of TensorFlow Variables\n",
    "print(layer.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing this allows you to implement your own training routine based on a TensorFlow optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two : Using Keras models with TensorFlow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Keras Sequential model for use in a TensorFlow workflow\n",
    "\n",
    "If you want to reuse a Keras Sequential model in another TensorFlow project, here is how to proceed :<br><br>\n",
    "\n",
    "First of all, please note that, if yout pre-trained weights include convolutions that were trained ith Theano, you will need to flip the convolution kernels when loadin the weights. Theano and TensorFlow are implementing convolution in a different way<br><br>\n",
    "\n",
    "Let's say that you are starting from the following Keras model. You want to modify it, so it takes as input a specific TensorFlow tensor : my_input_tensor.<br>\n",
    "This input tensor could be a data feeder operator, for instance, or the output of a previous TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import InputLayer\n",
    "from keras.models import Sequential\n",
    "\n",
    "# this is our initial Keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=784))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a Sequential model on top of a custom TensorFlow placeholder, use \"keras.layers.InputLayer\", then build the rest of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import InputLayer\n",
    "\n",
    "# Instanciate a layer for the input\n",
    "layer = Dense(32)\n",
    "\n",
    "# Call a layer instance for the input\n",
    "blah = layer(x)\n",
    "\n",
    "# this is the modified Keras model\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_tensor=blah,\n",
    "                    input_shape=(None, 784)))\n",
    "\n",
    "# and this is the rest of the model\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That can be usefull to know how to save and load the weights or models<br>\n",
    "first the weights only can be saved this way :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load this file , you build your model, then :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another saving technique \"model.save(filepath)\" will save the architecture of the model (allowing to re-create it), the weights of the model, the training configuration, and the state of the optimizer (allowing to resume training exactly where you left it) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is only for getting the current directory for saving the file\n",
    "import os\n",
    "filepath = os.path.abspath(os.path.curdir)\n",
    "\n",
    "model.save(filepath + '\\\\my_full_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load this saved model, you would use the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "new_model = load_model(filepath + '\\\\my_full_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last one is for saving the architecture of the model, using Json. with \"model.to_json()\" method that create a string. The string can be loaded by calling the \"model_from_json(string)\" method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our model, we can now collecting the Sequential model's output tensor :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor = model.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now add new TensorFlow operators on top of output_tensor, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling a Keras model on a TensorFlow tensor\n",
    "\n",
    "A Keras model acts like a layer, and thus can be called on TensorFlow tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=784))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# this works! \n",
    "x = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "y = model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
