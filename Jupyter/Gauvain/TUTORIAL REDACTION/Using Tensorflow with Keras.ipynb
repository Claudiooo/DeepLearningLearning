{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  A little introduction to Keras, using TensorFlow\n",
    "    \n",
    " Keras is a simple and high-level definition interface \n",
    " that make the use of TensorFlow more easier<br><br>\n",
    "\n",
    "<font color=red size=2 ><B><i>!! This tutorial assumes that you have configured Keras\n",
    " to use the TensorFlow backend</font><B></i></br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    \n",
    "## Part One : Calling Keras layers on TensorFlow tensors\n",
    "\n",
    " __We will start with a simple example of MNIST digits classification.<br>\n",
    " We will build a TensorFlow digits classifier using a stack of Keras Dense layers__\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to create a Tensorflow session and registering it with Keras.<br>\n",
    "* Keras will use this session to initialize all internal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now building our classifier exactly like we would do with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder that will contain our input digits, as flat vectors\n",
    "img = tf.placeholder(tf.float32, shape=(None, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Keras layers can be called on TensorFlow tensors :<br><br>\n",
    " *Using Keras layers will speed up the model definition process*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# Fully-connected layer with 128 unis\n",
    "# and ReLU activation (Rectified Linear Unit)\n",
    "x = Dense(128, activation='relu')(img)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "\n",
    "# Output layer with 10 units and a softmax activation\n",
    "preds = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the placeholder for the labels and the loss function to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "from keras.objectives import categorical_crossentropy\n",
    "loss = tf.reduce_mean(categorical_crossentropy(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training part, with a TensorFlow optimizer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "#Initialization of all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "#Run training loop\n",
    "with sess.as_default():\n",
    "    for i in range(100):\n",
    "        batch = mnist_data.train.next_batch(50)\n",
    "        train_step.run(feed_dict={img: batch[0], labels: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import categorical_accuracy as accuracy\n",
    "acc_value = accuracy(labels, preds)\n",
    "with sess.as_default():\n",
    "    print (acc_value.eval(feed_dict={img: mnist_data.test.images,\n",
    "                                    labels: mnist_data.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used Keras as a syntaxical shortcut to generate an operator that just map some input tensors to some output tensors.\n",
    "The optimization is done via a TensorFlow native optimizer rather than a Keras one and no Keras model have been used.<br><br>\n",
    "\n",
    "Concerning the optimizers performances, there is little speed differences between Keras and TensorFlow use. Keras seems to be faster than TensorFlow nativ optimizer on most cases, but the differences is only from 5 to 10%.\n",
    "So for most of the projects, it doesn't really matter which one you choose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some different behaviors during training and testing phases\n",
    "\n",
    "Some Keras layers (like Dropout or BatchNormalization) have different behaviors at training time and testing time.<br>\n",
    "It's possible to know if a layer uses the \"leaarning phase\" (train/test) by printin the layer.uses_learning_phase variable. A boolean that is set to True if the layer has a different behavior on training mode and testin mode, and False otherwise.<br><br>\n",
    "\n",
    "If the model use such layers, you need to specify the value of the learning phase as part of feed_dict. That way the model knows wether to apply the layer.<br><br>\n",
    "\n",
    "The Keras learnin phase is accessible via the Keras backend. <br>\n",
    "Like this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"keras_learning_phase:0\", dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "print (K.learning_phase())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the use of learning phase, write the value 1 for training mode or 0 for testing mode, to feed_dict :<br>\n",
    "*(here we have added \"session=sess\" because run() need a default session to work)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train mode\n",
    "train_step.run(feed_dict={img: batch[0], labels: batch[1],\n",
    "                          K.learning_phase(): 1}, session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code to add the Dropout layer to our previous MNIST example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  1.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "\n",
    "img = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "x = Dense(128, activation='relu')(img)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(10, activation='softmax')(x)\n",
    "\n",
    "loss = tf.reduce_mean(categorical_crossentropy(labels, preds))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init_op)\n",
    "with sess.as_default():\n",
    "    for i in range(100):\n",
    "        batch = mnist_data.train.next_batch(50)\n",
    "        train_step.run(feed_dict={img: batch[0],\n",
    "                                  labels: batch[1],\n",
    "                                  K.learning_phase(): 1})\n",
    "\n",
    "acc_value = accuracy(labels, preds)\n",
    "with sess.as_default():\n",
    "    print (acc_value.eval(feed_dict={img: mnist_data.test.images,\n",
    "                                    labels: mnist_data.test.labels,\n",
    "                                    K.learning_phase(): 0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! *Nexts codes snipets are here to explain the usage. But if you execute them you need to include theses :*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compatibility with name scopes and device scopes\n",
    "<br>\n",
    "Keras layers and models are fully compatible with TensorFlow name scopes. For instance, consider the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "with tf.name_scope('block1'):\n",
    "    y = LSTM(32, name='mylstm')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of our new LSTM layer will now be named block1/mylstm_W_i, block1/mylstm_U_i, and so on...\n",
    "\n",
    "Similarly, devices scopes would work like this :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "    y = LSTM(32)(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compatibility with graph scopes\n",
    "\n",
    "Any Keras layer or model that you define inside a TensorFlow graph scope will be created as part of the specified graph, with all he's variables an operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "\n",
    "my_graph = tf.Graph()\n",
    "with my_graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "    # LSTM operators and variables are part of the Graph\n",
    "    y = LSTM(32)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compatibility with variables scopes\n",
    "\n",
    "Variables sharing should be done via calling multiple instance of the same Keras layer, because the TenorFlow variable scope will have NO effect on a Keras model or layer.<br>\n",
    "More informations on wheight sharing with Keras could be found on the \"Wheight Sharing\" section of the functional API guide.<br><br>\n",
    "\n",
    "Here is an exemple of weight sharing by reusing the same layer instance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Keras layer\n",
    "lstm = LSTM(32)\n",
    "\n",
    "# Instantiate 2 TensorFlow placeholders\n",
    "x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "\n",
    "# Encoding the 2 tensors with the same LSTM weights\n",
    "x_encoded = lstm(x)\n",
    "y_encoded = lstm(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colleting trainable weights and state updates\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
